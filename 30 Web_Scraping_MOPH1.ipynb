{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Web_Scraping_MOPH version.1\n",
    "\n",
    "I tried to learn how to do Web scraping, this is my first time.\n",
    "\n",
    "***Reference: https://bigdata.go.th/big-data-101/data-science/data-scraping-part-1/***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_content(i_page):\n",
    "    url = f'http://203.157.10.8/hcode_2020/query_list.php?pageNum_rsList={i_page}&maincode=&oldcode=&mainname=&mincode=&deptcode=&typecode=&level_hospital_id=&status=&servicelevel=&provcode=&zone12=&p=3&Submit=%E0%B8%84%E0%B9%89%E0%B8%99%E0%B8%AB%E0%B8%B2%E0%B8%82%E0%B9%89%E0%B8%AD%E0%B8%A1%E0%B8%B9%E0%B8%A5&totalRows_rsList=28169'\n",
    "    source = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(source.content, 'lxml', from_encoding='utf-8')\n",
    "    div = soup.find('main', id='main')\n",
    "    table = div.find('table', class_='table table-bordered')\n",
    "    rows = table.find_all('tr')\n",
    "    \n",
    "    ls = [] # list to store text of all elements in all the rows.\n",
    "    for i, row in enumerate(rows):\n",
    "        # Each element is enclosed by either <th> or <td> tag.\n",
    "        if i==0:\n",
    "            elements = row.find_all('th')\n",
    "        else:\n",
    "            elements = row.find_all('td')\n",
    "        \n",
    "        # list to store text of all the elements in this row:\n",
    "        ls_elements_in_row = []\n",
    "        for element in elements:\n",
    "            text = element.text\n",
    "            ls_elements_in_row += [text]\n",
    "        \n",
    "        ls += [ls_elements_in_row]\n",
    "    \n",
    "    df = pd.DataFrame(ls[1::])\n",
    "    df.columns = ls[0]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot get data from page 0\n",
      "Cannot get data from page 1\n",
      "Cannot get data from page 2\n",
      "Cannot get data from page 3\n",
      "Cannot get data from page 4\n",
      "Cannot get data from page 5\n",
      "Cannot get data from page 6\n",
      "Cannot get data from page 7\n",
      "Cannot get data from page 8\n",
      "Cannot get data from page 9\n"
     ]
    }
   ],
   "source": [
    "df_all = pd.DataFrame()\n",
    "for i_page in range(0, 10):\n",
    "    try:\n",
    "        df = get_table_content(i_page)\n",
    "        df_all = pd.concat([df_all, df], axis=0, sort=False)\n",
    "        print(f'Done getting data from page {i_page}')\n",
    "    except:\n",
    "        print(f'Cannot get data from page {i_page}')\n",
    "\n",
    "#df_all.to_excel('table_id.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
